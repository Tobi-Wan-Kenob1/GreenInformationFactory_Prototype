{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419fea9d-3f5f-442e-af68-98444f39c7f5",
   "metadata": {},
   "source": [
    "Imports - config - repo paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8565f431-6d30-4d3b-a91d-9210bb68faea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'source': 'zenodo',\n",
       "  'doi': '10.5281/zenodo.16256961',\n",
       "  'raw_filename': 'BioFairNet_Pilot1_Testrun.csv',\n",
       "  'separator': ';',\n",
       "  'encoding': 'utf-8',\n",
       "  'dummy_mode': False},\n",
       " {'features': ['time', 'temperature', 'Stiring'],\n",
       "  'target': 'pressure',\n",
       "  'random_seed': 42,\n",
       "  'splits': {'holdout_fraction': 0.2, 'train_fraction_within_train': 0.8},\n",
       "  'models_default': ['linreg', 'rf', 'gbr', 'svr', 'mlp'],\n",
       "  'gridsearch': {'cv_folds': 5,\n",
       "   'n_jobs': -1,\n",
       "   'refit_metric': 'rmse',\n",
       "   'scoring': ['rmse', 'r2']}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "repo_root = Path.cwd().parents[0]\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.append(str(repo_root))\n",
    "\n",
    "from helper.utils import (\n",
    "    find_repo_root,\n",
    "    load_pipeline_config,\n",
    "    save_run_log,\n",
    "    ensure_dir,\n",
    "    sha256_file\n",
    ")\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "cfg = load_pipeline_config(repo_root)\n",
    "\n",
    "raw_dir = repo_root / cfg[\"paths\"][\"raw_dir\"]\n",
    "processed_dir = repo_root / cfg[\"paths\"][\"processed_dir\"]\n",
    "\n",
    "ensure_dir(processed_dir)\n",
    "\n",
    "cfg[\"dataset\"], cfg[\"ml\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0f740-b19f-451f-b3c0-fcb9173d56f0",
   "metadata": {},
   "source": [
    "### Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50bd6189-ccb5-4154-9dcb-759a9d3c1c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (9670, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (min)</th>\n",
       "      <th>Temperature (°C)</th>\n",
       "      <th>Stiring</th>\n",
       "      <th>Pressure (bar)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14:12:16</td>\n",
       "      <td>114,8</td>\n",
       "      <td>0,1</td>\n",
       "      <td>0,7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14:12:17</td>\n",
       "      <td>114,9</td>\n",
       "      <td>0,1</td>\n",
       "      <td>0,7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14:12:18</td>\n",
       "      <td>114,9</td>\n",
       "      <td>0,1</td>\n",
       "      <td>0,7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14:12:19</td>\n",
       "      <td>114,9</td>\n",
       "      <td>0,1</td>\n",
       "      <td>0,7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14:12:20</td>\n",
       "      <td>114,9</td>\n",
       "      <td>0,1</td>\n",
       "      <td>0,7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Time (min) Temperature (°C) Stiring Pressure (bar)\n",
       "0   14:12:16            114,8     0,1            0,7\n",
       "1   14:12:17            114,9     0,1            0,7\n",
       "2   14:12:18            114,9     0,1            0,7\n",
       "3   14:12:19            114,9     0,1            0,7\n",
       "4   14:12:20            114,9     0,1            0,7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_filename = cfg[\"dataset\"][\"raw_filename\"]\n",
    "sep = cfg[\"dataset\"].get(\"separator\", \";\")\n",
    "\n",
    "raw_path = raw_dir / raw_filename\n",
    "assert raw_path.exists(), f\"Raw file not found: {raw_path}\"\n",
    "\n",
    "df = pd.read_csv(raw_path, sep=sep)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee66bc-8f0c-4bd1-936a-4c719b3306f6",
   "metadata": {},
   "source": [
    "### Basic cleaning & standardization\n",
    "This handles:\n",
    "- deicmal commas --> dots\n",
    "- numeric coercion\n",
    "- NaN truncation\n",
    "- column name normalization\n",
    "\n",
    "prints the available columns and lets the user pick the feature columns + target column (with sensible defaults). Then it writes those choices back into `metadata/prepared_schema.json` (and optionally updates `pipeline_config.json`).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b> Depending on the dataset, you may do additional operations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d4dddf3-1635-4f19-ba6a-9b71b68e04e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF shape (9670, 4)\n",
      "Available columns:\n",
      "  [0] Time (min)\n",
      "  [1] Temperature (°C)\n",
      "  [2] Stiring\n",
      "  [3] Pressure (bar)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter TIME column name exactly as shown (or leave empty if none):  Time (min)\n",
      "Enter FEATURE column indices (comma-separated, e.g., 1,2,3):  1,2\n",
      "Enter TARGET column index (single number):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Selected:\n",
      "Time: Time (min)\n",
      "Features: ['Temperature (°C)', 'Stiring']\n",
      "Target: Pressure (bar)\n",
      "work shape before time handling: (9670, 4)\n",
      "Time parsing NaN ratio: 0.308 (0.0 is perfect)\n",
      "\n",
      "✅ Cleaned working frame shape: (9225, 4)\n",
      "   Temperature (°C)  Stiring  Pressure (bar)  time_s\n",
      "0             114.8      0.1             0.7     0.0\n",
      "1             114.9      0.1             0.7     1.0\n",
      "2             114.9      0.1             0.7     2.0\n",
      "3             114.9      0.1             0.7     3.0\n",
      "4             114.9      0.1             0.7     4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Keep original column names but strip whitespace\n",
    "df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "print(\"DF shape\", df.shape)\n",
    "print(\"Available columns:\")\n",
    "for i, c in enumerate(df.columns):\n",
    "    print(f\"  [{i}] {c}\")\n",
    "\n",
    "# ---- Interactive selection ----\n",
    "time_col = input(\"\\nEnter TIME column name exactly as shown (or leave empty if none): \").strip()\n",
    "feat_idx = input(\"Enter FEATURE column indices (comma-separated, e.g., 1,2,3): \").strip()\n",
    "tgt_idx  = input(\"Enter TARGET column index (single number): \").strip()\n",
    "\n",
    "\n",
    "if time_col and time_col not in df.columns:\n",
    "    raise ValueError(f\"Time column '{time_col}' not found. Available columns: {list(df.columns)}\")\n",
    "\n",
    "feat_idx = [int(x.strip()) for x in feat_idx.split(\",\") if x.strip() != \"\"]\n",
    "tgt_idx  = int(tgt_idx)\n",
    "\n",
    "features = [df.columns[i] for i in feat_idx]\n",
    "target   = df.columns[tgt_idx]\n",
    "\n",
    "required_cols = ([time_col] if time_col else []) + features + [target]\n",
    "work = df[required_cols].copy()\n",
    "\n",
    "print(\"\\n✅ Selected:\")\n",
    "print(\"Time:\", time_col if time_col else \"(none)\")\n",
    "print(\"Features:\", features)\n",
    "print(\"Target:\", target)\n",
    "\n",
    "print(\"work shape before time handling:\", work.shape)\n",
    "# ---- TIME handling (hh:mm:ss → seconds; rebuild series starting at 0) ----\n",
    "if time_col:\n",
    "    col_data = work.loc[:, time_col]\n",
    "\n",
    "    # If duplicate columns exist, take the first\n",
    "    if isinstance(col_data, pd.DataFrame):\n",
    "        print(\"⚠️ Duplicate time columns detected, using the first one.\")\n",
    "        col_data = col_data.iloc[:, 0]\n",
    "\n",
    "    # Ensure we actually have rows\n",
    "    if len(col_data) == 0:\n",
    "        raise ValueError(\"No rows available after selecting required columns. Check your column indices / filtering.\")\n",
    "\n",
    "    s = col_data.astype(str).str.strip()\n",
    "\n",
    "    t = pd.to_timedelta(s, errors=\"coerce\")\n",
    "\n",
    "    # Diagnostics\n",
    "    na_ratio = float(t.isna().mean())\n",
    "    print(f\"Time parsing NaN ratio: {na_ratio:.3f} (0.0 is perfect)\")\n",
    "\n",
    "    if na_ratio > 0.95:\n",
    "        bad = s[t.isna()].head(10).tolist()\n",
    "        raise ValueError(\n",
    "            f\"Time parsing failed for >95% of rows in '{time_col}'. \"\n",
    "            f\"Examples: {bad}\"\n",
    "        )\n",
    "\n",
    "    t_sec = t.dt.total_seconds()\n",
    "\n",
    "    # dt between consecutive rows\n",
    "    dt = t_sec.diff()\n",
    "\n",
    "    # if dt <= 0 (wrap/reset), mark as NaN\n",
    "    dt = dt.where(dt > 0)\n",
    "\n",
    "    # choose default dt from data; fallback to 15s\n",
    "    default_dt = dt.dropna().median()\n",
    "    if pd.isna(default_dt) or default_dt <= 0:\n",
    "        default_dt = 15.0\n",
    "\n",
    "    dt = dt.fillna(default_dt)\n",
    "\n",
    "    # only set first element if dt not empty\n",
    "    if len(dt) > 0:\n",
    "        dt.iloc[0] = 0.0\n",
    "    else:\n",
    "        # extreme edge case: no dt computed\n",
    "        dt = pd.Series([0.0])\n",
    "\n",
    "    work[\"time_s\"] = dt.cumsum().astype(float)\n",
    "\n",
    "    # drop original time column\n",
    "    work = work.drop(columns=[time_col])\n",
    "\n",
    "    # ensure features list updated\n",
    "    features = [c for c in features if c != time_col]\n",
    "    features = [\"time_s\"] + features\n",
    "\n",
    "\n",
    "# ---- Numeric cleanup ONLY for feature/target columns (exclude time strings) ----\n",
    "num_cols = features + [target]\n",
    "\n",
    "for col in num_cols:\n",
    "    # Allow commas as decimal separators; keep minus signs; strip spaces\n",
    "    work[col] = (\n",
    "        work[col]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    work[col] = pd.to_numeric(work[col], errors=\"coerce\")\n",
    "\n",
    "# ---- Drop rows where target is NaN, and where ALL features are NaN ----\n",
    "work = work.dropna(subset=[target])\n",
    "work = work.dropna(subset=features, how=\"all\")\n",
    "work = work.reset_index(drop=True)\n",
    "\n",
    "print(\"\\n✅ Cleaned working frame shape:\", work.shape)\n",
    "print(work.head())\n",
    "\n",
    "# Make downstream cells use this cleaned df\n",
    "df = work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c6018-d030-402c-8183-c2f73b589790",
   "metadata": {},
   "source": [
    "### Train/Test/Validation split (80/20 + 80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd54e3df-0a26-4011-b6d6-2489fb404d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (5904, 3)\n",
      "Test: (1476, 3)\n",
      "Validation: (1845, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "seed = cfg[\"ml\"][\"random_seed\"]\n",
    "\n",
    "# Holdout split (80/20)\n",
    "X_train_full, X_val, y_train_full, y_val = train_test_split(\n",
    "    X, y, test_size=cfg[\"ml\"][\"splits\"][\"holdout_fraction\"], random_state=seed\n",
    ")\n",
    "\n",
    "# Inner split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    test_size=1 - cfg[\"ml\"][\"splits\"][\"train_fraction_within_train\"],\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape)\n",
    "print(\"Test:\", X_test.shape)\n",
    "print(\"Validation:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3902e71-9513-4f1a-b5b1-348c39dd775a",
   "metadata": {},
   "source": [
    "### Save processed datasets ###\n",
    "Directory Structure:\n",
    "\n",
    "`data/processed/` <br>\n",
    "> `Train/` <br>\n",
    ">     `Test/` <br>\n",
    ">      `Validation/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "738e8437-c1d8-45db-9ab6-e3d6ff145481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved processed datasets\n"
     ]
    }
   ],
   "source": [
    "train_dir = processed_dir / \"Train\"\n",
    "test_dir = processed_dir / \"Test\"\n",
    "val_dir = processed_dir / \"Validation\"\n",
    "\n",
    "ensure_dir(train_dir)\n",
    "ensure_dir(test_dir)\n",
    "ensure_dir(val_dir)\n",
    "\n",
    "X_train.to_csv(train_dir / \"X_train.csv\", index=False)\n",
    "y_train.to_csv(train_dir / \"y_train.csv\", index=False)\n",
    "\n",
    "X_test.to_csv(test_dir / \"X_test.csv\", index=False)\n",
    "y_test.to_csv(test_dir / \"y_test.csv\", index=False)\n",
    "\n",
    "X_val.to_csv(val_dir / \"X_val.csv\", index=False)\n",
    "y_val.to_csv(val_dir / \"y_val.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved processed datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec47bf-618c-4b99-8566-d40a84421765",
   "metadata": {},
   "source": [
    "### Save schema and hashes\n",
    "\n",
    "Important for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d668204-4405-476f-adb6-da9c8364cd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema written: /home/097e80f6-6687-4e65-aab6-9abf7b887006/GreenInformationFactory_Prototype/metadata/prepared_schema.json\n",
      "✅ Hashes written: /home/097e80f6-6687-4e65-aab6-9abf7b887006/GreenInformationFactory_Prototype/metadata/prepared_hashes.json\n"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "    \"features\": features,\n",
    "    \"target\": target,\n",
    "    \"rows_total\": len(df),\n",
    "    \"splits\": {\n",
    "        \"train\": len(X_train),\n",
    "        \"test\": len(X_test),\n",
    "        \"validation\": len(X_val)\n",
    "    },\n",
    "    \"source_file\": str(raw_path.relative_to(repo_root)),\n",
    "    \"files\": {\n",
    "        \"X_train\": \"data/processed/Train/X_train.csv\",\n",
    "        \"y_train\": \"data/processed/Train/y_train.csv\",\n",
    "        \"X_test\": \"data/processed/Test/X_test.csv\",\n",
    "        \"y_test\": \"data/processed/Test/y_test.csv\",\n",
    "        \"X_val\": \"data/processed/Validation/X_val.csv\",\n",
    "        \"y_val\": \"data/processed/Validation/y_val.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "schema_path = repo_root / \"metadata\" / \"prepared_schema.json\"\n",
    "schema_path.write_text(json.dumps(schema, indent=2))\n",
    "print(\"✅ Schema written:\", schema_path)\n",
    "\n",
    "hashes = {k: sha256_file(repo_root / v) for k, v in schema[\"files\"].items()}\n",
    "\n",
    "hash_path = repo_root / \"metadata\" / \"prepared_hashes.json\"\n",
    "hash_path.write_text(json.dumps(hashes, indent=2))\n",
    "print(\"✅ Hashes written:\", hash_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94731fe-84a1-4e0e-ae7b-5e71c7c9da94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-greeninfo",
   "language": "python",
   "name": "conda-env-.conda-greeninfo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
